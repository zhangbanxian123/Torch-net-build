{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "print('torch.version:',torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# torch 包函数\n",
    "### 提供了多为张量的数据结构以及其上的多种数学操作\n",
    "### 这里只列举一些搭建网络时常用的张量操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# torch.numel(input)\n",
    "# 返回张量中的元素个数\n",
    "a = torch.randn(1,2,3,4,5)\n",
    "num = torch.numel(a)\n",
    "print(num)\n",
    "a = torch.zeros(4,4)\n",
    "num = torch.numel(a)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建张量操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "[-1  2  3]\n"
     ]
    }
   ],
   "source": [
    "# torch.for_numpy(ndarray)\n",
    "# 将numpy数组转换为张量，返回的张量与numpy数组共享内存空间，修改其中一个，另一个也会变化\n",
    "a = np.array([1,2,3])\n",
    "t = torch.from_numpy(a)\n",
    "print(t)\n",
    "t[0]=-1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "#torch.ones(*sizes, out=None)\n",
    "#返回一个全为1的张量，形状为size\n",
    "t = torch.ones(2, 3)\n",
    "print(t)\n",
    "t = torch.ones(5)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2073, 0.2724, 0.6357, 0.8404])\n",
      "tensor([[0.6538, 0.8037, 0.3969],\n",
      "        [0.7419, 0.6290, 0.8508]])\n"
     ]
    }
   ],
   "source": [
    "# torch.rand(*size, out=None)\n",
    "# fanhui yige zhangliang ,baohan qujian [0,1)的均匀分布中抽取的一组随机数， 形状由size定义\n",
    "t = torch.rand(4)\n",
    "print(t)\n",
    "t = torch.rand(2,3)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.9027,  0.2626, -1.7842,  1.7237])\n",
      "tensor([[-0.0926, -0.1079, -0.3787],\n",
      "        [ 2.4367,  0.1863, -1.8357]])\n"
     ]
    }
   ],
   "source": [
    "# torch.randn(*sizes, out=None)\n",
    "# 返回一个张量，包含了从标准正态分布中抽取一组随机数， 形状由size定义\n",
    "t = torch.randn(4)\n",
    "print(t)\n",
    "t = torch.randn(2, 3)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 0, 2, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# torch.randperm(n, out=None)\n",
    "# 给定参数n， 返回一个从0到n-1的随机整数序列\n",
    "t = torch.randperm(5)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# torch.zeros(*sizes, out=None)\n",
    "#返回一个全为标量0的张量， 形状由size定义\n",
    "t = torch.zeros(2,3)\n",
    "print(t)\n",
    "t = torch.zeros(5)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 索引、切片、连接、换位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2658,  0.6475, -0.1520],\n",
      "        [ 0.0197, -0.1919,  2.7836]])\n",
      "tensor([[ 0.2658,  0.6475, -0.1520],\n",
      "        [ 0.0197, -0.1919,  2.7836],\n",
      "        [ 0.2658,  0.6475, -0.1520],\n",
      "        [ 0.0197, -0.1919,  2.7836],\n",
      "        [ 0.2658,  0.6475, -0.1520],\n",
      "        [ 0.0197, -0.1919,  2.7836]])\n",
      "tensor([[ 0.2658,  0.6475, -0.1520,  0.2658,  0.6475, -0.1520,  0.2658,  0.6475,\n",
      "         -0.1520],\n",
      "        [ 0.0197, -0.1919,  2.7836,  0.0197, -0.1919,  2.7836,  0.0197, -0.1919,\n",
      "          2.7836]])\n"
     ]
    }
   ],
   "source": [
    "# torch.cat(inputs, dimension=0)\n",
    "# 给定维度上对输入张量序列进行连接操作\n",
    "# 可以看做是torch.split()和torch.chunk()的反操作\n",
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "y = torch.cat((x,x,x), dim=0)\n",
    "print(y)\n",
    "y = torch.cat((x,x,x), dim=1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5200, -0.0619, -1.0714],\n",
      "        [ 1.1593, -0.8721,  1.0234],\n",
      "        [-0.4907,  0.8121, -0.1660]])\n",
      "tensor([[ 1.5200, -0.0619, -1.0714]]) tensor([[ 1.1593, -0.8721,  1.0234]]) tensor([[-0.4907,  0.8121, -0.1660]])\n",
      "tensor([[ 1.5200],\n",
      "        [ 1.1593],\n",
      "        [-0.4907]]) \n",
      " tensor([[-0.0619],\n",
      "        [-0.8721],\n",
      "        [ 0.8121]]) \n",
      " tensor([[-1.0714],\n",
      "        [ 1.0234],\n",
      "        [-0.1660]])\n"
     ]
    }
   ],
   "source": [
    "# torch.chunk(tensor, chunks, dim=0)\n",
    "# 在给定维度上将输入张量进行分块\n",
    "# chunks： 分块的个数\n",
    "t = torch.randn(3, 3)\n",
    "print(t)\n",
    "a, b, c = torch.chunk(t, 3, dim=0)\n",
    "print(a,b,c)\n",
    "a, b, c = torch.chunk(t, 3, dim=1)\n",
    "print(a,'\\n',b,'\\n',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4790, -0.9932, -0.3516,  0.1689,  0.2450,  0.1955],\n",
      "        [ 0.3751, -1.7559, -0.2864, -0.9003, -0.0486,  1.5217],\n",
      "        [-0.4601, -0.7660, -0.7716,  0.8151, -2.6800, -0.0153],\n",
      "        [-0.5875, -0.1617,  0.3872,  0.4164,  0.1402, -1.4469],\n",
      "        [ 0.3918,  1.8908, -0.2315, -0.9515,  0.4350,  1.8377],\n",
      "        [-0.8952, -1.9530, -1.9013, -2.0954,  0.7590, -1.1621]])\n",
      "tensor([[ 1.4790, -0.9932, -0.3516,  0.1689,  0.2450,  0.1955],\n",
      "        [ 0.3751, -1.7559, -0.2864, -0.9003, -0.0486,  1.5217],\n",
      "        [-0.4601, -0.7660, -0.7716,  0.8151, -2.6800, -0.0153]]) \n",
      " tensor([[-0.5875, -0.1617,  0.3872,  0.4164,  0.1402, -1.4469],\n",
      "        [ 0.3918,  1.8908, -0.2315, -0.9515,  0.4350,  1.8377],\n",
      "        [-0.8952, -1.9530, -1.9013, -2.0954,  0.7590, -1.1621]])\n",
      "tensor([[ 1.4790, -0.9932, -0.3516],\n",
      "        [ 0.3751, -1.7559, -0.2864],\n",
      "        [-0.4601, -0.7660, -0.7716],\n",
      "        [-0.5875, -0.1617,  0.3872],\n",
      "        [ 0.3918,  1.8908, -0.2315],\n",
      "        [-0.8952, -1.9530, -1.9013]]) \n",
      " tensor([[ 0.1689,  0.2450,  0.1955],\n",
      "        [-0.9003, -0.0486,  1.5217],\n",
      "        [ 0.8151, -2.6800, -0.0153],\n",
      "        [ 0.4164,  0.1402, -1.4469],\n",
      "        [-0.9515,  0.4350,  1.8377],\n",
      "        [-2.0954,  0.7590, -1.1621]])\n"
     ]
    }
   ],
   "source": [
    "# torch.split(input, split_size, dim=0)\n",
    "# 将输入张量分割相等形状的chunks， 如果沿指定dim的张量形状不能被整分，则最后一个小块会小于其他块\n",
    "t = torch.randn(6,6)\n",
    "print(t)\n",
    "a, b = torch.split(t, 3, dim=0)\n",
    "print(a, '\\n',b)\n",
    "a, b = torch.split(t, 3, dim=1)\n",
    "print(a, '\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3616,  0.0512,  0.8111, -0.7402],\n",
      "        [-1.0830, -1.5905, -0.0566,  1.4523],\n",
      "        [-0.9813, -1.0178,  0.9778, -0.2004]])\n",
      "tensor([[-0.3616,  0.0512,  0.8111, -0.7402],\n",
      "        [-0.9813, -1.0178,  0.9778, -0.2004]])\n",
      "tensor([[-0.3616,  0.8111],\n",
      "        [-1.0830, -0.0566],\n",
      "        [-0.9813,  0.9778]])\n"
     ]
    }
   ],
   "source": [
    "# torch.index_select(input, dim, index, out=None)\n",
    "# 沿着指定维度对输入进行切片\n",
    "# 返回的张量不予原始张量共享内存\n",
    "t = torch.randn(3, 4)\n",
    "print(t)\n",
    "indices = torch.LongTensor([0,2])\n",
    "x = torch.index_select(t, 0, indices)\n",
    "print(x)\n",
    "x = torch.index_select(t, 1, indices)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]])\n",
      "tensor([[0, 0],\n",
      "        [1, 1],\n",
      "        [2, 2]])\n"
     ]
    }
   ],
   "source": [
    "# torch.nonzero(input, out=None)\n",
    "# 返回一个包含input中非零元素索引的张量\n",
    "t = torch.eye(3,4)\n",
    "print(t)\n",
    "index = torch.nonzero(t)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2, 1, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 1, 2, 1, 2])\n",
      "torch.Size([2, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# torch.squeze(input , dim=None, out=None)\n",
    "# 将输入张量形状中的1去除并返回，如输入形状是（aX1XbX1XcX1XdX1）输出为aXbXcXd\n",
    "# 当指定dim时，那么挤压操作只在给定维度上作用\n",
    "# 返回张量与输入张量共享内存\n",
    "\n",
    "t = torch.zeros(2,1,2,1,2)\n",
    "print(t.size())\n",
    "y = torch.squeeze(t)\n",
    "print(y.size())\n",
    "y = torch.squeeze(t, dim=0)\n",
    "print(y.size())\n",
    "y = torch.squeeze(t, dim=1)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.unsqueeze(input, dim, out=None)\n",
    "# 返回一个新的张量，对输入的指定位置插入维度1\n",
    "# 返回的张量与原始张量共享内存\n",
    "t = torch.randn(2, 3)\n",
    "print(t.size())\n",
    "y = torch.unsqueeze(t, dim=0)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1541,  1.5992],\n",
      "         [-0.0729, -0.2575]],\n",
      "\n",
      "        [[ 1.1541,  1.5992],\n",
      "         [-0.0729, -0.2575]],\n",
      "\n",
      "        [[ 1.1541,  1.5992],\n",
      "         [-0.0729, -0.2575]]])\n",
      "torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# torch.stack(seqyence, dim=0)\n",
    "# sequence: 待连接的张量序列\n",
    "# 与 torch.cat不同的是，stack会在指定dim创建新的维度\n",
    "a = torch.randn(2,2)\n",
    "b = torch.randn(2,2)\n",
    "t = torch.stack((a, a, a), dim=0)\n",
    "print(t)\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2412, -0.7360, -1.0196],\n",
      "        [-2.2201,  0.1677,  1.4594]])\n",
      "tensor([[-0.2412, -2.2201],\n",
      "        [-0.7360,  0.1677],\n",
      "        [-1.0196,  1.4594]])\n"
     ]
    }
   ],
   "source": [
    "# torch.t(input, out=None)\n",
    "# 输入一个2维矩阵，并转置，可视为torch.transpose(input,0,1)的简写函数\n",
    "t =torch.randn(2, 3)\n",
    "print(t)\n",
    "y = torch.t(t)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3204,  1.1682, -0.0872, -1.9974],\n",
      "         [-1.3310, -0.9180,  0.7218, -0.8850],\n",
      "         [-0.0611, -1.2719,  0.7856, -0.5773]],\n",
      "\n",
      "        [[-0.3415,  0.5506, -1.9949, -1.6552],\n",
      "         [ 1.5703, -0.0792, -0.9050, -2.3225],\n",
      "         [-0.1709,  0.8616,  1.6834,  0.1522]]])\n",
      "tensor([[[-0.3204,  1.1682, -0.0872, -1.9974],\n",
      "         [-0.3415,  0.5506, -1.9949, -1.6552]],\n",
      "\n",
      "        [[-1.3310, -0.9180,  0.7218, -0.8850],\n",
      "         [ 1.5703, -0.0792, -0.9050, -2.3225]],\n",
      "\n",
      "        [[-0.0611, -1.2719,  0.7856, -0.5773],\n",
      "         [-0.1709,  0.8616,  1.6834,  0.1522]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.transpose(input, dim0, dim1, out=None)\n",
    "# 返回输入矩阵的转置，交换维度dim0与dim1，\n",
    "# 输出张量与输出张量共享内存\n",
    "t = torch.randn(2, 3, 4)\n",
    "print(t)\n",
    "y = torch.transpose(t, 0, 1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch.nn包相关模块介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class torch.nn.Module() \n",
    "# 所有网络的基类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules也可以抱哈其他Modules，允许使用树结构嵌入他们，也可以将子模块复制给模型属性\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 3)\n",
    "        self.conv2 = nn.Conv2d(20, 20, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        return F.relu(self.conv2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpu(): 将所有模型参数和buffers复制到cpu\n",
    "# gpu(): 将所有模型参数个buffers复制到gpu\n",
    "# eval(): 将模型设置为evalution模式，仅仅当模型中有Droupout和BatchNorm时才会有影响\n",
    "# train(): 将模型设置为train模式，仅仅当模型中有Droupout和BatchNorm时才会有影响\n",
    "# parameters(): 返回一个包含模型所有参数的迭代\n",
    "# 一般用来当做oprimizer的参数\n",
    "# zero_grad()：将module中的所有模型参数的梯度设置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class torch.nn.Seqiential()\n",
    "# 一个时序容器，Modules会议他们传入的顺序被添加到容器中，\n",
    "model = nn.Sequential(\n",
    "                nn.Conv2d(1, 20, 5),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(20, 20, 5),\n",
    "                nn.ReLU()\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 33, 26, 50])\n"
     ]
    }
   ],
   "source": [
    "# class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, \n",
    "#                                         padding=0, dilation=1, groups=1, bias=True)\n",
    "# Parameters:\n",
    "# diliation:卷积核元素之间的间距，当大于1时即形成空洞卷积\n",
    "# groups: 控制输入和输出之间的连接： group=1，输出是所有的输入的卷积；\n",
    "#              group=2，此时相当于有并排的两个卷积层，每个卷积层计算输入通道的一半，\n",
    "#             并且产生的输出是输出通道的一半，随后将这两个输出连接起来。\n",
    "m = nn.Conv2d(16, 33, (3, 5), stride=2, padding=(4, 2), dilation=(3, 1))\n",
    "input = torch.autograd.Variable(torch.randn(20, 16, 50, 100))\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 6, 6])\n",
      "torch.Size([1, 16, 12, 12])\n",
      "torch.Size([1, 16, 11, 11])\n"
     ]
    }
   ],
   "source": [
    "# class torch.nn.ConvTranspose2d(input_channels, output_channels, kenel_size，\n",
    "#                           stride=1, padding=0, output_padding=0, groups=1, bias=True)\n",
    "# 二维的转置卷积操作，科士威解卷积操作，但不是真正的解卷积操作\n",
    "input = torch.autograd.Variable(torch.randn(1, 16, 12, 12))\n",
    "downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)\n",
    "upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)\n",
    "h = downsample(input)\n",
    "print(h.size())\n",
    "h1 = upsample(h, output_size=input.size())\n",
    "h2= upsample(h)\n",
    "print(h1.size())\n",
    "print(h2.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 16, 25, 16])\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.MaxPool2d(kenel_size, stride=None, padding=0,\n",
    "#                    dilation=1, return_indices=False, ceil_mode=False)\n",
    "# 最大二维池化操作，很好理解\n",
    "# parameters：\n",
    "# return_indices: 如果为True，会返回输出最大值的序号，对上采样有帮助\n",
    "# ceil_mode: 如果为True，计算输出信号大小的时候，会向上取整，代替默认的向下取整\n",
    "m = nn.MaxPool2d(2)\n",
    "input = torch.autograd.Variable(torch.randn(20, 16, 50, 32))\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n",
      "tensor([[[[ 0.,  0.,  0.,  0.],\n",
      "          [ 0.,  6.,  0.,  8.],\n",
      "          [ 0.,  0.,  0.,  0.],\n",
      "          [ 0., 14.,  0., 16.]]]])\n",
      "tensor([[[[ 0.,  0.,  0.,  0.,  0.],\n",
      "          [ 6.,  0.,  8.,  0.,  0.],\n",
      "          [ 0.,  0.,  0., 14.,  0.],\n",
      "          [16.,  0.,  0.,  0.,  0.],\n",
      "          [ 0.,  0.,  0.,  0.,  0.]]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.MaxUppool2d(kernel_size, stride=None. padding=0)\n",
    "# MaxPool2d的逆过程， 并不是完整的逆过程，因为在maxpool2d的过程中，一些最大值已经丢失\n",
    "# 注意：Maxpool2d可以将多个输入大小映射到相同的输出大小，因此反演过程可能会变得模棱两可，\n",
    "#       为了适应这一点，可以在调用中将输出大小output_size作为额外的参数传入\n",
    "# 输入：input：需要转换的tensor\n",
    "#        indices：Maxpool2d的索引号\n",
    "#        output_size:一个指定输出大小的torch.size\n",
    "pool = nn.MaxPool2d(2, return_indices=True)\n",
    "unpool = nn.MaxUnpool2d(2)\n",
    "input = Variable(torch.Tensor([[[[1, 2, 3, 4],\n",
    "                                 [5, 6, 7, 8],\n",
    "                                [9, 10, 11, 12],\n",
    "                                [13, 14, 15, 16]]]]))\n",
    "print(input.size())\n",
    "output, indices = pool(input)\n",
    "output1 = unpool(output, indices)\n",
    "print(output1)\n",
    "\n",
    "output2 = unpool(output, indices, output_size=torch.Size([1, 1, 5, 5]))\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 非线性激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0375,  0.7673, -0.6387],\n",
      "        [ 2.2014,  0.4968, -0.2370]])\n",
      "tensor([[0.2791, 0.5790, 0.1419],\n",
      "        [0.7879, 0.1433, 0.0688]])\n"
     ]
    }
   ],
   "source": [
    "# class torch.nn.Softmax()\n",
    "# 对n维张量运用siftmax函数，将张量的每个元素缩放到0~1，且和为1\n",
    "m = nn.Softmax(dim=1)\n",
    "input = Variable(torch.randn(2, 3))\n",
    "output = m(input)\n",
    "print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5915,  1.2821])\n",
      "tensor([0.0000, 1.2821])\n",
      "tensor([0.0000, 1.2821])\n"
     ]
    }
   ],
   "source": [
    "# class torch.nn.ReLU(inplace=False)\n",
    "# inplace:选择是否进行覆盖运算\n",
    "m = nn.ReLU(inplace=True)\n",
    "input = Variable(torch.randn(2))\n",
    "print(input)\n",
    "output = m(input)\n",
    "print(output)\n",
    "print(F.relu(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0156,  0.6847])\n",
      "tensor([0.0000, 0.6847])\n",
      "tensor([0.0000, 0.6847])\n"
     ]
    }
   ],
   "source": [
    "# class torch.nn.ReLU6(implace=False)\n",
    "# in_place:选择是否进行覆盖运算，若果覆盖，可以节省内存\n",
    "# x = min(max(0, x), 6) 最大激活值为6\n",
    "m = nn.ReLU6()\n",
    "input = Variable(torch.randn(2))\n",
    "print(input)\n",
    "output = m(input)\n",
    "print(output)\n",
    "print(F.relu(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1633, 1.0373])\n",
      "tensor([0.8221, 0.7768])\n"
     ]
    }
   ],
   "source": [
    "# class torch.nn.Tanh\n",
    "# 对每个输入元素， 计算f(x)=(e^x-e^-x)/(e^x+e^-x)\n",
    "# 将输入限制在0~1\n",
    "m = nn.Tanh()\n",
    "input = Variable(torch.randn(2))\n",
    "print(input)\n",
    "print(m(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0698,  0.5344],\n",
      "        [ 0.5783,  0.6368]])\n",
      "tensor([[0.1614, 0.4744],\n",
      "        [0.8386, 0.5256]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input = Variable(torch.randn(2,2))\n",
    "print(input)\n",
    "print(F.softmax(input, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.6036, -0.1050, -0.5292],\n",
      "          [ 0.3276,  0.9532, -0.8595]],\n",
      "\n",
      "         [[-0.6410, -0.7502, -0.0283],\n",
      "          [-1.2451, -0.2500, -0.4774]]],\n",
      "\n",
      "\n",
      "        [[[-1.5284, -1.1040, -0.3194],\n",
      "          [-1.1344, -1.2175,  1.3156]],\n",
      "\n",
      "         [[-0.6666,  1.5257,  0.4784],\n",
      "          [ 0.6649,  2.0937, -0.3757]]]])\n",
      "tensor([[[[ 0.0720,  0.0155, -0.0183],\n",
      "          [ 0.0500,  0.0998, -0.0446]],\n",
      "\n",
      "         [[-0.2334, -0.2715, -0.0194],\n",
      "          [-0.4444, -0.0968, -0.1763]]],\n",
      "\n",
      "\n",
      "        [[[-0.0979, -0.0641, -0.0016],\n",
      "          [-0.0665, -0.0731,  0.1287]],\n",
      "\n",
      "         [[-0.2423,  0.5232,  0.1575],\n",
      "          [ 0.2226,  0.7216, -0.1408]]]], grad_fn=<NativeBatchNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "# class torch.nn.BatchNorm2d(num_features,eps=1e-5,momentum=0.1,\n",
    "#                           affine=True)\n",
    "# 在每个batch数据中，计算输入各个维度的均值与标准差，\n",
    "# num_features:来自期望输入的特征数\n",
    "#  eps:为保证数据稳定性（坟墓不能趋近或取0）给分母加上一个值， 默认1e-5\n",
    "# momentum:动态均值可动态方差所使用的栋梁，默认0.1\n",
    "# affine:为True时，给改成添加可学习的仿射变换参数\n",
    "m = nn.BatchNorm2d(2)\n",
    "m1 = nn.BatchNorm2d(2, affine=True)\n",
    "input = Variable(torch.randn(2, 2, 2, 3))\n",
    "output1 = m(input)\n",
    "output2 = m1(input)\n",
    "print(input)\n",
    "print(output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linner layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20])\n",
      "torch.Size([2, 30])\n"
     ]
    }
   ],
   "source": [
    "# class torch.nn.Linear(in_features, out_features, bias=True)\n",
    "# 对输入数据做线性变换y=Ax+b\n",
    "m = nn.Linear(20, 30)\n",
    "input = Variable(torch.randn(2, 20))\n",
    "output = m(input)\n",
    "print(input.size())\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000, -2.6564, -0.0000],\n",
      "         [ 1.1167,  0.0000, -1.3734],\n",
      "         [-0.0000, -0.8802,  0.0000],\n",
      "         [-0.1679, -0.0000,  0.8384],\n",
      "         [ 0.0000,  0.6846,  0.0000],\n",
      "         [-2.1018, -0.0000, -2.1762]],\n",
      "\n",
      "        [[-0.0000, -1.1254,  0.0000],\n",
      "         [ 1.4707,  0.0000,  0.4841],\n",
      "         [-0.0000,  3.8631,  0.0000],\n",
      "         [-1.0782,  0.0000, -0.9463],\n",
      "         [-0.0000, -3.2843,  0.0000],\n",
      "         [-1.6249, -0.0000,  1.2248]]])\n"
     ]
    }
   ],
   "source": [
    "# calss torch.nn.Dropout(p=0.2, inplace=False)\n",
    "# 随机将输入张量中部分元素置为0，对于每次前向调用，置0的元素都是随机的\n",
    "# p:将元素置0的概率\n",
    "# inplace:若为True，会在原地执行操作\n",
    "m = nn.Dropout(p=0.5)\n",
    "input = Variable(torch.randn(2, 6, 3))\n",
    "output = m(input)\n",
    "# print(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
      "         [ 5.5649,  0.3528,  1.0334],\n",
      "         [-0.0000,  0.0000,  0.0000],\n",
      "         [ 2.1818, -0.4656, -0.2783],\n",
      "         [-0.0000, -0.0000,  0.0000],\n",
      "         [-2.2832,  2.4742, -1.3990]],\n",
      "\n",
      "        [[-0.0000,  0.0000, -0.0000],\n",
      "         [ 0.3037, -4.2412, -1.9647],\n",
      "         [-0.0000, -0.0000,  0.0000],\n",
      "         [ 0.0320, -4.7925, -1.3204],\n",
      "         [-0.0000,  0.0000, -0.0000],\n",
      "         [ 2.8148, -2.5035, -1.5152]]])\n"
     ]
    }
   ],
   "source": [
    "# class torch.nn.Dropout2d()\n",
    "# 与Dropout不同的是，Droupout是对每个元素进行随机的置0，而Dropout2d是对dim=0的通道全部进行置0\n",
    "# 像在论文Efficient Object Localization Using Convolutional Networks，\n",
    "# 如果特征图中相邻像素是强相关的（在前几层卷积层很常见），那么iid dropout不会归一化激活，而只会降低学习率。\n",
    "n = nn.Dropout2d(p=0.5)\n",
    "input = Variable(torch.randn(2, 6, 3))\n",
    "output = n(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class torch.nn.L1Loss(size_average=True)\n",
    "# 创建一个衡量输入与目标y之间的绝对值的平均值的标准差\n",
    "# 如果size_average=False，那么求出来的绝对值的和不会除以n\n",
    "\n",
    "# class torch.nn.MSELoss(size_average=True)\n",
    "# 衡量预测与目标之间的均方误差\n",
    "\n",
    "# class torch.nn.NLLLoss(weight=None, sizeO_average=True)\n",
    "# 用于训练一个n类的分类器\n",
    "\n",
    "# class torch.nn.CrossEntropyLoss(weight=None, size_average=True)\n",
    "# 将LogSoftmax和NLLLoss集成到一个类中\n",
    "# 通常用于训练一个多分类器\n",
    "\n",
    "# class torch.nn.BCELoss(weight=None, size_average=True)\n",
    "# 计算预测与目标的二进制交叉熵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPsampling Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 2.],\n",
      "          [3., 4.]]]])\n",
      "tensor([[[[1., 1., 2., 2.],\n",
      "          [1., 1., 2., 2.],\n",
      "          [3., 3., 4., 4.],\n",
      "          [3., 3., 4., 4.]]]])\n"
     ]
    }
   ],
   "source": [
    "# class torch.nn.UpsamplingNearest2d(size = None, scale_factor=None)\n",
    "# 对于多通道输入进行最近邻上采样\n",
    "# 可以通过size和scale_factor指定上采样后的大小\n",
    "# 当给定size时，输出图想大小将是size\n",
    "input = Variable(torch.Tensor([[[[1,2],[3,4]]]]))\n",
    "print(input)\n",
    "upsam = torch.nn.UpsamplingNearest2d(scale_factor=2)\n",
    "output = upsam(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 2.],\n",
      "          [3., 4.]]]])\n",
      "tensor([[[[1.0000, 1.3333, 1.6667, 2.0000],\n",
      "          [1.6667, 2.0000, 2.3333, 2.6667],\n",
      "          [2.3333, 2.6667, 3.0000, 3.3333],\n",
      "          [3.0000, 3.3333, 3.6667, 4.0000]]]])\n"
     ]
    }
   ],
   "source": [
    "# class torch.nn.UpsampingBilinear2d(size=None, scale_factor=None)\n",
    "# 对多通道输入进行线性上采样\n",
    "# 同样通过size或scale_factor来决定输出大小\n",
    "input = Variable(torch.Tensor([[[[1,2],[3,4]]]]))\n",
    "print(input)\n",
    "upsam = torch.nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "output = upsam(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mutil-GPU layers\n",
    "# class torch.nn.DataParallel(module, devices_ids=None, output_device=None, deim=0)\n",
    "# 在模块级别上实现数据并行\n",
    "# 此容器将mini-batch划分到不同的设备上来实现戈丁module的并行，在forwaed过程中，module会在每个设备上都复制一遍，每个副本都会处理部分输入，\n",
    "# 在backward过程中，副本级的梯度会累加到原始的module上\n",
    "# catch的大小应该大于所使用的GPU数量，还应该是GPU数量的整数倍\n",
    "# 参数说明：\n",
    "# module：要被并行执行的模型\n",
    "# device_ids：CUDA设备，默认所有设备\n",
    "# output_device:输出设备，默认为device_ids[0]\n",
    "\n",
    "net = torch.nn.DataParallel(model, device_ids=[1,2,3])\n",
    "output = net(input_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特别说明\n",
    "## 关于torch.nn 和torch.nn.functional的区别与联系\n",
    "### 在写 PyTorch 代码时，我们会发现在 torch.nn.xxx 和 torch.nn.functional.xxx 中有一些功能重复的操作，比如卷积、激活、池化。这些操作有什么不同？各有什么用处？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先我们来看一下torch.nn.Conv2d和torch.nn.functional.conv2d是如何定义的\n",
    "# torch.nn.Conv2d\n",
    "class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "# torch.nn.functional.conv2d\n",
    "torch.nn.functional.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) → Tensor\n",
    "\n",
    "# 我们可以发现，nn.Conv2d是一个类，而nn.functional.conv2d是一个函数\n",
    "# 此外还有以下几点不同：\n",
    "# 1.两者调用方式不同，调用nn.xxx时要先在里面传入超参数，然后在将数据以函数调用的方式传入nn.xxx，如\n",
    "input = torch.randn(64, 3, 224, 224)\n",
    "conv = nn.Conv2d(in_channel=3, out_chaneels=64, kernel_size=4, paddin=1)\n",
    "out = conv(input)\n",
    "# 而nn.functional.xxx则要同时输入数据和weight，bias等参数\n",
    "weight = torch.randn(64, 3, 3, 3)\n",
    "bias = torch.randn(64)\n",
    "out = nn.functional.conv2d(inputs, weight, bias, padding=1)\n",
    "# 2.nn.xxx能够放在nn.Sequential中，而nn.functional.xxx不行\n",
    "# 3.nn.functional.xxx需要自己定义weight，每次调用时都要手动传入weight和bias，而nn.xxx则不用\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self)：\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(1, 16, 5, padding=0)\n",
    "        self.relu_1 = nn.ReLU(inplace=True)\n",
    "        self.maxpool_1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(16, 32, 5, padding=0)\n",
    "        self.relu_2 = nn.ReLU(inplace=True)\n",
    "        self.maxpool_2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.liner = nn.Linear(4*4*32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = self.maxpool_1(self.relu_1(self.conv2d_1))\n",
    "        out = self.maxpool_2(self.relu_2(self.conv2d_2))\n",
    "        out = self.liner(out.view(x.size(0), -1))\n",
    "        return out\n",
    "\n",
    "# torch.nn.functional 定义一个相同的CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv_1_weight = nn.Parameter(torch.randn(16, 1, 5, 5))\n",
    "        self.bias_1_weight = nn.Parameter(torch.randn(16))\n",
    "        \n",
    "        self.conv_2_weight = nn.Parameter(torch.randn(32, 16, 5, 5))\n",
    "        self.bias_2_weight = nn.Parameter(torch.randn(32))\n",
    "        \n",
    "        self.linear_weight = nn.Parameter(torch.randn(4 * 4 * 32, 10))\n",
    "        self.bias_weight = nn.Parameter(torch.randn(10))\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out = F.conv2d(x, self.conv_1_weight, self.bias_1_weight)\n",
    "        out = F.conv2d(out, self.conv_2_weight, self.bias_2_weight)\n",
    "        out = F.linear(out.view(x.size(0), -1), self.linear_weight, self.bias_weight)     \n",
    "#4 .在使用Dropout时，推荐使用nn.xxx，因为一般只有训练时才使用 Dropout，\n",
    "# 在验证或测试时不需要使用 Dropout。使用 nn.Dropout时，如果调用 model.eval() ，\n",
    "# 模型的 Dropout 层都会关闭；但如果使用 nn.functional.dropout，在调用 model.eval() 时，不会关闭 Dropout。\n",
    "# 5.我们想要自定义卷积核时，是不能使用torch.nn.ConvNd 的，\n",
    "# 因为它里面的权重都是需要学习的参数，没有办法自行定义。但是，我们可以使用 torch.nn.functional.conv2d()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet网络搭建实例\n",
    "LeNet网络结构\n",
    "![jupyter](https://img-blog.csdn.net/20180302201820978?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvamVyeWplcnlqZXJ5/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "假设激活函数使用relu，卷积核大小为5x5，下面我们开始尝试搭建LeNet吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self, LeNet).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(1, 6, 5, padding=1)\n",
    "        self.max_pool_1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(6, 16, 5, padding=1)\n",
    "        self.max_pool_2 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.max_pool_1(F.relu(self.conv_1(x)))\n",
    "        x = self.max_pool_2(F.relu(self.conv_2(x)))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16网络搭建实例\n",
    "VGG16网络结构图如下：\n",
    "![jupyter](https://img-blog.csdnimg.cn/2018121616365865.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3poYW5naGFvMzM4OQ==,size_16,color_FFFFFF,t_70)\n",
    "假设激活函数为relu，卷积核大小3x3,有batch-norm操作，下面我们开始愉快地尝试搭建VGG16吧  \n",
    "更多实例请参考pytorch vgg实现源码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self, VGG16).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(nn.Conv2d(3, 64, 3, padding=1),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(64, 64, 3, padding=1),\n",
    "                                    nn.BatchNorm2d(64),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.MaxPooling2d(2))\n",
    "        self.layer2 = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1),\n",
    "                                    nn.BatchNorm2d(128),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(128, 128, 3, padding=1),\n",
    "                                    nn.BatchNorm2d(128),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.MaxPooling2d(2))\n",
    "        self.layer3 = nn.Sequential(nn.Conv2d(128, 256, 3, padding=1),\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(256, 256, 3, padding=1),\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(256, 256, 3, padding=1),\n",
    "                                    nn.BatchNorm2d(256),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.MaxPooling2d(2))\n",
    "        self.layer4 = nn.Sequential(nn.Conv2d(256, 512, 3, padding=1),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(512, 512, 3, padding=1),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(512, 512, 3, padding=1),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.MaxPooling2d(2))\n",
    "        self.layer5 = nn.Sequential(nn.Conv2d(512, 512, 3, padding=1),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(512, 512, 3, padding=1),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.Conv2d(512, 512, 3, padding=1),\n",
    "                                    nn.BatchNorm2d(512),\n",
    "                                    nn.ReLU(inplace=True),\n",
    "                                    nn.MaxPooling2d(2))\n",
    "        self.fc1 = nn.Linear(7*7*512, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.LInear(4096, 1000)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.softmax(self.fc3(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet18 网络搭建实例\n",
    "ResNet18网络结构如下：\n",
    "![jupyter](https://img-blog.csdn.net/20180426215052446?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1bnFpYW5kZTg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)\n",
    "ResNet有几种类型，分别为18、34、50、101、152个隐藏层，常用的有ResNet50、ResNet101、ResNet152，这里因为只是做个示范，所以以最简单ResNet18为例。\n",
    "ResNet有两种跳跃结构，分别用于ResNet18、ResNet34和ResNet50、ResNet101、ResNet152，结构分别为:  \n",
    "![jupyter](https://img-blog.csdn.net/20180114183212429?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGFucmFuMg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)  \n",
    "这两种结构分别针对ResNet34（左图）和ResNet50/101/152（右图），一般称整个结构为一个”building block“。其中左图称为“basic design”右图称为”bottleneck design”，右图目的一目了然，就是为了降低参数的数目，第一个1x1的卷积把256维channel降到64维，然后在最后通过1x1卷积恢复，整体上用的参数数目：1x1x256x64 + 3x3x64x64 + 1x1x64x256 = 69632，而不使用bottleneck的话就是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，差了16.94倍。  \n",
    "不同的ResNet结构如下图：  \n",
    "![jupyter](https://img-blog.csdnimg.cn/20190202101501192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dpbnljZw==,size_16,color_FFFFFF,t_70)\n",
    "假设激活函数为relu，卷积核大小3x3,有batch-norm操作，下面我们开始愉快地尝试搭建resnet18吧  \n",
    "说明：ResNet有很多旁路支线可以将输入直接连到后面的层，使得后面的层可以直接学习残差，简化了学习难度。传统的卷积层和全连接层在信息传递时，或多或少会存在信息丢失，损耗等问题。ResNet将输入信息绕道传到输出，保护了信息的完整性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 官方实现\n",
    "# 用于ResNet18和34的残差块，用的是2个3x3的卷积\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 经过处理后的x要与x的维度相同(尺寸和深度)\n",
    "        # 如果不相同，需要添加卷积+BN来变换为同一维度\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "# 用于ResNet50,101和152的残差块，用的是1x1+3x3+1x1的卷积\n",
    "class Bottleneck(nn.Module):\n",
    "    # 前面1x1和3x3卷积的filter个数相等，最后1x1卷积是其expansion倍\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
