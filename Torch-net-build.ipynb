{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.version: 1.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "print('torch.version:',torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# torch 包函数\n",
    "### 提供了多为张亮的数据结构以及其上的多种数学操作\n",
    "### 这里只列举一些搭建网络时常用的张量操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# torch.numel(input)\n",
    "# 返回张量中的元素个数\n",
    "a = torch.randn(1,2,3,4,5)\n",
    "num = torch.numel(a)\n",
    "print(num)\n",
    "a = torch.zeros(4,4)\n",
    "num = torch.numel(a)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建张量操作 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "[-1  2  3]\n"
     ]
    }
   ],
   "source": [
    "# torch.for_numpy(ndarray)\n",
    "# 将numpy数组转换为张量，返回的张量与numpy数组共享内存空间，修改其中一个，另一个也会变化\n",
    "a = np.array([1,2,3])\n",
    "t = torch.from_numpy(a)\n",
    "print(t)\n",
    "t[0]=-1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "#torch.ones(*sizes, out=None)\n",
    "#返回一个全为1的张量，形状为size\n",
    "t = torch.ones(2, 3)\n",
    "print(t)\n",
    "t = torch.ones(5)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2073, 0.2724, 0.6357, 0.8404])\n",
      "tensor([[0.6538, 0.8037, 0.3969],\n",
      "        [0.7419, 0.6290, 0.8508]])\n"
     ]
    }
   ],
   "source": [
    "# torch.rand(*size, out=None)\n",
    "# fanhui yige zhangliang ,baohan qujian [0,1)的均匀分布中抽取的一组随机数， 形状由size定义\n",
    "t = torch.rand(4)\n",
    "print(t)\n",
    "t = torch.rand(2,3)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.9027,  0.2626, -1.7842,  1.7237])\n",
      "tensor([[-0.0926, -0.1079, -0.3787],\n",
      "        [ 2.4367,  0.1863, -1.8357]])\n"
     ]
    }
   ],
   "source": [
    "# torch.randn(*sizes, out=None)\n",
    "# 返回一个张量，包含了从标准正态分布中抽取一组随机数， 形状由size定义\n",
    "t = torch.randn(4)\n",
    "print(t)\n",
    "t = torch.randn(2, 3)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 0, 2, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# torch.randperm(n, out=None)\n",
    "# 给定参数n， 返回一个从0到n-1的随机整数序列\n",
    "t = torch.randperm(5)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# torch.zeros(*sizes, out=None)\n",
    "#返回一个全为标量0的张量， 形状由size定义\n",
    "t = torch.zeros(2,3)\n",
    "print(t)\n",
    "t = torch.zeros(5)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 索引、切片、连接、换位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2658,  0.6475, -0.1520],\n",
      "        [ 0.0197, -0.1919,  2.7836]])\n",
      "tensor([[ 0.2658,  0.6475, -0.1520],\n",
      "        [ 0.0197, -0.1919,  2.7836],\n",
      "        [ 0.2658,  0.6475, -0.1520],\n",
      "        [ 0.0197, -0.1919,  2.7836],\n",
      "        [ 0.2658,  0.6475, -0.1520],\n",
      "        [ 0.0197, -0.1919,  2.7836]])\n",
      "tensor([[ 0.2658,  0.6475, -0.1520,  0.2658,  0.6475, -0.1520,  0.2658,  0.6475,\n",
      "         -0.1520],\n",
      "        [ 0.0197, -0.1919,  2.7836,  0.0197, -0.1919,  2.7836,  0.0197, -0.1919,\n",
      "          2.7836]])\n"
     ]
    }
   ],
   "source": [
    "# torch.cat(inputs, dimension=0)\n",
    "# 给定维度上对输入张量序列进行连接操作\n",
    "# 可以看做是torch.split()和torch.chunk()的反操作\n",
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "y = torch.cat((x,x,x), dim=0)\n",
    "print(y)\n",
    "y = torch.cat((x,x,x), dim=1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5200, -0.0619, -1.0714],\n",
      "        [ 1.1593, -0.8721,  1.0234],\n",
      "        [-0.4907,  0.8121, -0.1660]])\n",
      "tensor([[ 1.5200, -0.0619, -1.0714]]) tensor([[ 1.1593, -0.8721,  1.0234]]) tensor([[-0.4907,  0.8121, -0.1660]])\n",
      "tensor([[ 1.5200],\n",
      "        [ 1.1593],\n",
      "        [-0.4907]]) \n",
      " tensor([[-0.0619],\n",
      "        [-0.8721],\n",
      "        [ 0.8121]]) \n",
      " tensor([[-1.0714],\n",
      "        [ 1.0234],\n",
      "        [-0.1660]])\n"
     ]
    }
   ],
   "source": [
    "# torch.chunk(tensor, chunks, dim=0)\n",
    "# 在给定维度上将输入张量进行分块\n",
    "# chunks： 分块的个数\n",
    "t = torch.randn(3, 3)\n",
    "print(t)\n",
    "a, b, c = torch.chunk(t, 3, dim=0)\n",
    "print(a,b,c)\n",
    "a, b, c = torch.chunk(t, 3, dim=1)\n",
    "print(a,'\\n',b,'\\n',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4790, -0.9932, -0.3516,  0.1689,  0.2450,  0.1955],\n",
      "        [ 0.3751, -1.7559, -0.2864, -0.9003, -0.0486,  1.5217],\n",
      "        [-0.4601, -0.7660, -0.7716,  0.8151, -2.6800, -0.0153],\n",
      "        [-0.5875, -0.1617,  0.3872,  0.4164,  0.1402, -1.4469],\n",
      "        [ 0.3918,  1.8908, -0.2315, -0.9515,  0.4350,  1.8377],\n",
      "        [-0.8952, -1.9530, -1.9013, -2.0954,  0.7590, -1.1621]])\n",
      "tensor([[ 1.4790, -0.9932, -0.3516,  0.1689,  0.2450,  0.1955],\n",
      "        [ 0.3751, -1.7559, -0.2864, -0.9003, -0.0486,  1.5217],\n",
      "        [-0.4601, -0.7660, -0.7716,  0.8151, -2.6800, -0.0153]]) \n",
      " tensor([[-0.5875, -0.1617,  0.3872,  0.4164,  0.1402, -1.4469],\n",
      "        [ 0.3918,  1.8908, -0.2315, -0.9515,  0.4350,  1.8377],\n",
      "        [-0.8952, -1.9530, -1.9013, -2.0954,  0.7590, -1.1621]])\n",
      "tensor([[ 1.4790, -0.9932, -0.3516],\n",
      "        [ 0.3751, -1.7559, -0.2864],\n",
      "        [-0.4601, -0.7660, -0.7716],\n",
      "        [-0.5875, -0.1617,  0.3872],\n",
      "        [ 0.3918,  1.8908, -0.2315],\n",
      "        [-0.8952, -1.9530, -1.9013]]) \n",
      " tensor([[ 0.1689,  0.2450,  0.1955],\n",
      "        [-0.9003, -0.0486,  1.5217],\n",
      "        [ 0.8151, -2.6800, -0.0153],\n",
      "        [ 0.4164,  0.1402, -1.4469],\n",
      "        [-0.9515,  0.4350,  1.8377],\n",
      "        [-2.0954,  0.7590, -1.1621]])\n"
     ]
    }
   ],
   "source": [
    "# torch.split(input, split_size, dim=0)\n",
    "# 将输入张量分割相等形状的chunks， 如果沿指定dim的张量形状不能被整分，则最后一个小块会小于其他块\n",
    "t = torch.randn(6,6)\n",
    "print(t)\n",
    "a, b = torch.split(t, 3, dim=0)\n",
    "print(a, '\\n',b)\n",
    "a, b = torch.split(t, 3, dim=1)\n",
    "print(a, '\\n',b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3616,  0.0512,  0.8111, -0.7402],\n",
      "        [-1.0830, -1.5905, -0.0566,  1.4523],\n",
      "        [-0.9813, -1.0178,  0.9778, -0.2004]])\n",
      "tensor([[-0.3616,  0.0512,  0.8111, -0.7402],\n",
      "        [-0.9813, -1.0178,  0.9778, -0.2004]])\n",
      "tensor([[-0.3616,  0.8111],\n",
      "        [-1.0830, -0.0566],\n",
      "        [-0.9813,  0.9778]])\n"
     ]
    }
   ],
   "source": [
    "# torch.index_select(input, dim, index, out=None)\n",
    "# 沿着指定维度对输入进行切片\n",
    "# 返回的张量不予原始张量共享内存\n",
    "t = torch.randn(3, 4)\n",
    "print(t)\n",
    "indices = torch.LongTensor([0,2])\n",
    "x = torch.index_select(t, 0, indices)\n",
    "print(x)\n",
    "x = torch.index_select(t, 1, indices)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]])\n",
      "tensor([[0, 0],\n",
      "        [1, 1],\n",
      "        [2, 2]])\n"
     ]
    }
   ],
   "source": [
    "# torch.nonzero(input, out=None)\n",
    "# 返回一个包含input中非零元素索引的张量\n",
    "t = torch.eye(3,4)\n",
    "print(t)\n",
    "index = torch.nonzero(t)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2, 1, 2])\n",
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 1, 2, 1, 2])\n",
      "torch.Size([2, 2, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "# torch.squeze(input , dim=None, out=None)\n",
    "# 将输入张量形状中的1去除并返回，如输入形状是（aX1XbX1XcX1XdX1）输出为aXbXcXd\n",
    "# 当指定dim时，那么挤压操作只在给定维度上作用\n",
    "# 返回张量与输入张量共享内存\n",
    "\n",
    "t = torch.zeros(2,1,2,1,2)\n",
    "print(t.size())\n",
    "y = torch.squeeze(t)\n",
    "print(y.size())\n",
    "y = torch.squeeze(t, dim=0)\n",
    "print(y.size())\n",
    "y = torch.squeeze(t, dim=1)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# torch.unsqueeze(input, dim, out=None)\n",
    "# 返回一个新的张量，对输入的指定位置插入维度1\n",
    "# 返回的张量与原始张量共享内存\n",
    "t = torch.randn(2, 3)\n",
    "print(t.size())\n",
    "y = torch.unsqueeze(t, dim=0)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.1541,  1.5992],\n",
      "         [-0.0729, -0.2575]],\n",
      "\n",
      "        [[ 1.1541,  1.5992],\n",
      "         [-0.0729, -0.2575]],\n",
      "\n",
      "        [[ 1.1541,  1.5992],\n",
      "         [-0.0729, -0.2575]]])\n",
      "torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# torch.stack(seqyence, dim=0)\n",
    "# sequence: 待连接的张量序列\n",
    "# 与 torch.cat不同的是，stack会在指定dim创建新的维度\n",
    "a = torch.randn(2,2)\n",
    "b = torch.randn(2,2)\n",
    "t = torch.stack((a, a, a), dim=0)\n",
    "print(t)\n",
    "print(t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2412, -0.7360, -1.0196],\n",
      "        [-2.2201,  0.1677,  1.4594]])\n",
      "tensor([[-0.2412, -2.2201],\n",
      "        [-0.7360,  0.1677],\n",
      "        [-1.0196,  1.4594]])\n"
     ]
    }
   ],
   "source": [
    "# torch.t(input, out=None)\n",
    "# 输入一个2维矩阵，并转置，可视为torch.transpose(input,0,1)的简写函数\n",
    "t =torch.randn(2, 3)\n",
    "print(t)\n",
    "y = torch.t(t)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3204,  1.1682, -0.0872, -1.9974],\n",
      "         [-1.3310, -0.9180,  0.7218, -0.8850],\n",
      "         [-0.0611, -1.2719,  0.7856, -0.5773]],\n",
      "\n",
      "        [[-0.3415,  0.5506, -1.9949, -1.6552],\n",
      "         [ 1.5703, -0.0792, -0.9050, -2.3225],\n",
      "         [-0.1709,  0.8616,  1.6834,  0.1522]]])\n",
      "tensor([[[-0.3204,  1.1682, -0.0872, -1.9974],\n",
      "         [-0.3415,  0.5506, -1.9949, -1.6552]],\n",
      "\n",
      "        [[-1.3310, -0.9180,  0.7218, -0.8850],\n",
      "         [ 1.5703, -0.0792, -0.9050, -2.3225]],\n",
      "\n",
      "        [[-0.0611, -1.2719,  0.7856, -0.5773],\n",
      "         [-0.1709,  0.8616,  1.6834,  0.1522]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.transpose(input, dim0, dim1, out=None)\n",
    "# 返回输入矩阵的转置，交换维度dim0与dim1，\n",
    "# 输出张量与输出张量共享内存\n",
    "t = torch.randn(2, 3, 4)\n",
    "print(t)\n",
    "y = torch.transpose(t, 0, 1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
